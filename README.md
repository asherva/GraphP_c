import pyodbc
import pandas as pd
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import subprocess

# ===== ×”×’×“×¨×•×ª ×—×™×‘×•×¨ ×œ-MSSQL =====
server = "SERVER_NAME"
database = "DB_NAME"
username = "USER"
password = "PASS"
table_name = "Sales"  # ×©× ×˜×‘×œ×ª ×”××›×™×¨×•×ª

# ===== ×—×™×‘×•×¨ ×œ××¡×“ ×”× ×ª×•× ×™× ×•×§×¨×™××ª ×˜×‘×œ×” =====
def load_sales_table():
    conn_str = (
        f"DRIVER={{ODBC Driver 17 for SQL Server}};"
        f"SERVER={server};DATABASE={database};UID={username};PWD={password}"
    )
    conn = pyodbc.connect(conn_str)
    query = f"SELECT * FROM {table_name}"
    df = pd.read_sql(query, conn)
    conn.close()
    return df

# ===== ×‘× ×™×™×ª ××™× ×“×§×¡ FAISS =====
def build_faiss_index(df):
    model = SentenceTransformer("all-MiniLM-L6-v2")  # ××•×“×œ embedding ××§×•××™
    texts = df.astype(str).agg(" ".join, axis=1).tolist()
    embeddings = model.encode(texts, convert_to_numpy=True)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index, texts, model

# ===== ×—×™×¤×•×© ×‘×˜×‘×œ×” =====
def search(query, index, texts, model, top_k=5):
    query_emb = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_emb, top_k)
    return [texts[i] for i in indices[0]]

# ===== ×©×œ×™×—×ª ×©××œ×” ×œ-Ollama =====
def ollama_generate(prompt, model="mistral"):
    cmd = ["ollama", "run", model]
    process = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    out, err = process.communicate(input=prompt)
    return out.strip()

# ===== ×¤×•× ×§×¦×™×” ×œ×©××œ×•×ª =====
def answer_question(query, index, texts, model):
    results = search(query, index, texts, model)
    context = "\n".join(results)
    prompt = (
        "×”× ×š ×¢×•×–×¨ ×—×›× ×œ× ×™×ª×•×— × ×ª×•× ×™ ××›×™×¨×•×ª. "
        "×”×©×ª××© ××š ×•×¨×§ ×‘× ×ª×•× ×™× ×”×‘××™× ×›×“×™ ×œ×¢× ×•×ª:\n"
        f"{context}\n\n"
        f"×©××œ×”: {query}\n"
        "×ª×©×•×‘×”:"
    )
    return ollama_generate(prompt, model="mistral")  # ××¤×©×¨ ×œ×”×—×œ×™×£ ×œ-llama3

# ===== ×”×¨×¦×” =====
if __name__ == "__main__":
    print("ğŸ“¥ ×˜×•×¢×Ÿ ××ª ×˜×‘×œ×ª ×”××›×™×¨×•×ª...")
    df = load_sales_table()
    index, texts, emb_model = build_faiss_index(df)
    print("âœ… ×”××¢×¨×›×ª ××•×›× ×”. ××¤×©×¨ ×œ×©××•×œ ×©××œ×•×ª!")

    while True:
        q = input("\n×”×›× ×¡ ×©××œ×” (××• 'exit' ×œ×™×¦×™××”): ")
        if q.lower() == "exit":
            break
        answer = answer_question(q, index, texts, emb_model)
        print("\nğŸ’¡ ×ª×©×•×‘×”:", answer)print(f"× ×˜×¢× ×• {len(df)} ×©×•×¨×•×ª ×•-{len(df.columns)} ×¢××•×“×•×ª.")

# ===== ×”××¨×ª ×©×•×¨×•×ª ×œ×˜×§×¡×˜×™× =====
def row_to_text(row):
    return "; ".join(f"{col}: {row[col]}" for col in df.columns)

docs = [row_to_text(r) for _, r in df.iterrows()]

# ===== ×™×¦×™×¨×ª Embeddings =====
EMBED_MODEL_NAME = "all-MiniLM-L6-v2"
embedder = SentenceTransformer(EMBED_MODEL_NAME)
embeddings = embedder.encode(docs, convert_to_numpy=True)

# ===== ×‘× ×™×™×ª ××™× ×“×§×¡ FAISS =====
dim = embeddings.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(embeddings)
print(f"××™× ×“×§×¡ FAISS × ×‘× ×” ×¢× {index.ntotal} ×¤×¨×™×˜×™×.")

# ===== ×˜×¢×™× ×ª ××•×“×œ ×©×¤×” ××§×•××™ =====
LOCAL_LLM_MODEL = "mistralai/Mistral-7B-Instruct-v0.1"  # ××¤×©×¨ ×œ×”×—×œ×™×£ ×‘××•×“×œ ××—×¨ ×©×”×•×¨×“×ª
llm = pipeline("text-generation", model=LOCAL_LLM_MODEL, device_map="auto")

# ===== ×¤×•× ×§×¦×™×™×ª ×—×™×¤×•×© =====
def search(query, top_k=5):
    q_emb = embedder.encode([query], convert_to_numpy=True)
    D, I = index.search(q_emb, top_k)
    results = []
    for dist, idx in zip(D[0], I[0]):
        results.append({
            "score": float(dist),
            "text": docs[idx],
            "row": df.iloc[idx].to_dict()
        })
    return results

# ===== ×¤×•× ×§×¦×™×” ×œ×¢× ×™×™×ª ×©××œ×•×ª (RAG) =====
def answer_question(query, top_k=5):
    results = search(query, top_k)
    context = "\n".join([r["text"] for r in results])
    prompt = (
        "×”× ×š ×¢×•×–×¨ ×—×›× ×œ××›×™×¨×•×ª. "
        "×”× ×” × ×ª×•× ×™ ×”×”×§×©×¨:\n"
        f"{context}\n"
        f"×©××œ×”: {query}\n"
        "×¢× ×” ×‘×¦×•×¨×” ××¤×•×¨×˜×ª ×•××‘×•×¡×¡×ª ×¢×œ ×”× ×ª×•× ×™× ×‘×œ×‘×“."
    )
    resp = llm(prompt, max_length=512, do_sample=False)
    return resp[0]["generated_text"]

# ===== ×××©×§ CLI =====
while True:
    q = input("\n×©××œ×” ('quit' ×œ×™×¦×™××”): ").strip()
    if q.lower() in ("quit", "exit"):
        break
    answer = answer_question(q, top_k=5)
    print("\n--- ×ª×©×•×‘×” ---")
    print(answer)
# ===== ×¤×•× ×§×¦×™×™×ª ×—×™×¤×•×© =====
def search(query, top_k=5):
    q_emb = model.encode([query], convert_to_numpy=True)
    D, I = index.search(q_emb, top_k)
    results = []
    for dist, idx in zip(D[0], I[0]):
        results.append({
            "score": float(dist),
            "row": df.iloc[idx].to_dict()
        })
    return results

# ===== × ×™×¡×•×™ ×—×™×¤×•×© =====
while True:
    q = input("\n×©××œ×” ××• ×—×™×¤×•×© ('quit' ×œ×™×¦×™××”): ").strip()
    if q.lower() in ("quit", "exit"):
        break
    hits = search(q, top_k=3)
    for h in hits:
        print(f"\n×¦×™×•×Ÿ: {h['score']:.4f}")
        for k, v in h["row"].items():
            print(f"{k}: {v}")
