""" Sales RAG (MSSQL + Ollama Embeddings + Ollama LLM) â€” 100% ×¤× ×™××™

×ª×œ×•×™×•×ª (×”×ª×§× ×” ×—×“-×¤×¢××™×ª): pip install pandas pyodbc faiss-cpu requests

× ×“×¨×© ×œ×”×ª×§×™×Ÿ ×•×œ×”×¨×™×¥ Ollama ××§×•××™×ª, ×•×œ×”×•×¨×™×“ ××ª ×”××•×“×œ×™×: ollama pull nomic-embed-text   # ××•×“×œ Embedding ollama pull llama3.1           # ××• mistral / gemma / llama3

×‘×¨×™×¨×ª ××—×“×œ: ×©×™××•×© ×‘-REST API ×”××§×•××™ ×©×œ Ollama (http://localhost:11434) ××™×Ÿ ×™×¦×™××” ×œ××™× ×˜×¨× ×˜ ××¢×‘×¨ ×œ×–×”. """

import os import json import time import struct from typing import List, Dict, Any, Tuple

import pandas as pd import numpy as np import pyodbc import faiss import requests

======================== ×”×’×“×¨×•×ª ========================

×—×™×‘×•×¨ ×œ××¡×“ ×”× ×ª×•× ×™× (×¢×“×›×Ÿ ×œ×¢×¨×›×™× ×©×œ×š)

MSSQL_SERVER   = os.getenv("MSSQL_SERVER", "localhost\SQLEXPRESS") MSSQL_DATABASE = os.getenv("MSSQL_DATABASE", "SalesDB") MSSQL_USERNAME = os.getenv("MSSQL_USERNAME", "sa") MSSQL_PASSWORD = os.getenv("MSSQL_PASSWORD", "YourStrong!Passw0rd") MSSQL_TABLE    = os.getenv("MSSQL_TABLE", "Sales")

Ollama

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434") EMBED_MODEL = os.getenv("EMBED_MODEL", "nomic-embed-text")  # ×××“ ×•×§×˜×•×¨ ××•×¤×™×™× ×™: 768/1024 (×ª×œ×•×™ ××•×“×œ) LLM_MODEL   = os.getenv("LLM_MODEL", "llama3.1")            # ××• "mistral", "llama3", "gemma" ×•×›×•'

×§×‘×¦×™ ××˜××•×Ÿ/××™× ×“×§×¡ (×œ× ×—×•×‘×”)

INDEX_DIR   = os.getenv("INDEX_DIR", "./rag_index") INDEX_FILE  = os.path.join(INDEX_DIR, "faiss.index") META_FILE   = os.path.join(INDEX_DIR, "meta.parquet") DIM_FILE    = os.path.join(INDEX_DIR, "dim.txt")

os.makedirs(INDEX_DIR, exist_ok=True)

======================== ×¢×–×¨×™ Ollama ========================

def ollama_embeddings(texts: List[str], model: str = EMBED_MODEL, timeout: int = 120) -> np.ndarray: """××‘×§×© embedding ×¢×‘×•×¨ ×¨×©×™××ª ×˜×§×¡×˜×™× ××”×©×¨×ª ×”××§×•××™ ×©×œ Ollama. ××©×ª××© ×‘-POST /api/embeddings. ××—×–×™×¨ np.ndarray ×‘×¦×•×¨×” (N, D). """ vectors = [] url = f"{OLLAMA_HOST}/api/embeddings" for t in texts: payload = {"model": model, "prompt": t} r = requests.post(url, json=payload, timeout=timeout) r.raise_for_status() data = r.json() vec = np.array(data.get("embedding", []), dtype=np.float32) if vec.size == 0: raise RuntimeError("Ollama ×”×—×–×™×¨ embedding ×¨×™×§. ×•×“× ×©×”××•×“×œ ×ª×•××š ×‘-embeddings.") vectors.append(vec) return np.vstack(vectors)

def ollama_generate(prompt: str, model: str = LLM_MODEL, temperature: float = 0.1, timeout: int = 300) -> str: """×™×•×¦×¨ ×ª×©×•×‘×” ××”-LLM ×”××§×•××™ ×‘-Ollama ×“×¨×š /api/generate (non-stream).""" url = f"{OLLAMA_HOST}/api/generate" payload = { "model": model, "prompt": prompt, "options": { "temperature": temperature, }, "stream": False } r = requests.post(url, json=payload, timeout=timeout) r.raise_for_status() data = r.json() # ×”×¤×•×¨××˜ ××—×–×™×¨ 'response' ×¢× ×”×˜×§×¡×˜ ×”××œ× ×›××©×¨ stream=False return data.get("response", "").strip()

======================== MSSQL ========================

def load_sales_table() -> pd.DataFrame: conn_str = ( f"DRIVER={{ODBC Driver 17 for SQL Server}};" f"SERVER={MSSQL_SERVER};DATABASE={MSSQL_DATABASE};UID={MSSQL_USERNAME};PWD={MSSQL_PASSWORD}" ) with pyodbc.connect(conn_str) as conn: df = pd.read_sql(f"SELECT * FROM {MSSQL_TABLE}", conn) # × ×™×§×•×™ ×‘×¡×™×¡×™ ×œ×©××•×ª ×¢××•×“×•×ª df.columns = [c.strip().replace(" ", "_") for c in df.columns] return df

======================== ×”××¨×” ×œ×˜×§×¡×˜×™× ========================

def row_to_text(row: pd.Series) -> str: # × ×™×ª×Ÿ ×œ×”×ª××™×: ×œ×‘×—×•×¨ ×¢××•×“×•×ª ××¨×›×–×™×•×ª ×‘×œ×‘×“, ××• ×œ×ª×¨×’× ×©××•×ª ×œ×¢×‘×¨×™×ª return "; ".join(f"{col}: {row[col]}" for col in row.index)

def df_to_texts(df: pd.DataFrame) -> List[str]: return [row_to_text(r) for _, r in df.iterrows()]

======================== FAISS ========================

def build_faiss(embeddings: np.ndarray) -> faiss.IndexFlatIP: # × ×©×ª××© ×‘-Inner Product ×¢× vectors ×× ×•×¨××œ×™× (×™×¢×™×œ ×œ×§×•×¡×™×™×Ÿ) dim = embeddings.shape[1] index = faiss.IndexFlatIP(dim) return index

def normalize_rows(x: np.ndarray) -> np.ndarray: norms = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12 return x / norms

def save_index(index: faiss.Index, df_meta: pd.DataFrame, dim: int): faiss.write_index(index, INDEX_FILE) df_meta.to_parquet(META_FILE, index=False) with open(DIM_FILE, "w", encoding="utf-8") as f: f.write(str(dim))

def load_index() -> Tuple[faiss.Index, pd.DataFrame, int]: index = faiss.read_index(INDEX_FILE) df_meta = pd.read_parquet(META_FILE) with open(DIM_FILE, "r", encoding="utf-8") as f: dim = int(f.read().strip()) return index, df_meta, dim

======================== RAG ========================

def ensure_index(df: pd.DataFrame, force_rebuild: bool = False) -> Tuple[faiss.Index, pd.DataFrame, int]: """×‘×•× ×” ××• ×˜×•×¢×Ÿ ××™× ×“×§×¡: ×××™×¨ ××ª ×”×©×•×¨×•×ª ×œ×˜×§×¡×˜×™×, ××¤×™×§ embeddings ×‘-Ollama, ×•×××—×¡×Ÿ ×‘-FAISS. ××—×–×™×¨ (index, df_meta, dim). """ if (not force_rebuild) and os.path.exists(INDEX_FILE) and os.path.exists(META_FILE) and os.path.exists(DIM_FILE): try: idx, meta, dim = load_index() return idx, meta, dim except Exception: pass

texts = df_to_texts(df)
print(f"×™×•×¦×¨ embeddings ×œ-{len(texts)} ×©×•×¨×•×ª... (×™×›×•×œ ×œ×§×—×ª ×–××Ÿ ×‘×¤×¢× ×”×¨××©×•× ×”)")
embs = ollama_embeddings(texts, EMBED_MODEL)  # ×¦×•×¨×” (N, D)
embs = normalize_rows(embs).astype(np.float32)

index = build_faiss(embs)
index.add(embs)

# meta × ×©××•×¨: ××™× ×“×§×¡-×©×•×¨×” ××§×•×¨×™ + ×˜×§×¡×˜ ×”××§×•×¨ (×œ× ×—×•×‘×” ×œ×©××•×¨ ×”×›×œ)
meta = pd.DataFrame({
    "row_id": np.arange(len(df), dtype=np.int32),
    "text": texts
})

save_index(index, meta, embs.shape[1])
return index, meta, embs.shape[1]

def search_similar(query: str, index: faiss.Index, df_meta: pd.DataFrame, top_k: int = 5) -> List[Dict[str, Any]]: q_emb = ollama_embeddings([query], EMBED_MODEL)  # (1, D) q_emb = normalize_rows(q_emb).astype(np.float32) D, I = index.search(q_emb, top_k)  # D: similarities, I: indices results = [] for score, idx in zip(D[0], I[0]): if idx == -1: continue results.append({ "score": float(score), "row_id": int(df_meta.loc[idx, "row_id"]), "text": df_meta.loc[idx, "text"], }) return results

def build_prompt(query: str, hits: List[Dict[str, Any]]) -> str: ctx = "\n".join(f"[DOC {i+1}] {h['text']}" for i, h in enumerate(hits)) prompt = ( "××ª/×” ×¢×•×–×¨/×ª × ×™×ª×•×— ××›×™×¨×•×ª ×¤× ×™××™/×ª. \n" "×¢× ×”/×™ ×‘×¢×‘×¨×™×ª, ××“×•×™×§, ×ª××¦×™×ª×™ ×›×©××¤×©×¨, ×•×”×¦×’/×™ ×—×™×©×•×‘×™× ×× ×¨×œ×•×•× ×˜×™. \n" "×”×¡×ª××š/×™ ××š ×•×¨×§ ×¢×œ ×”×”×§×©×¨ ×”×‘×: \n" f"{ctx}\n\n" f"×©××œ×”: {query}\n" "×ª×©×•×‘×”:" ) return prompt

def answer_question(query: str, index: faiss.Index, df_meta: pd.DataFrame, top_k: int = 6) -> Dict[str, Any]: hits = search_similar(query, index, df_meta, top_k=top_k) prompt = build_prompt(query, hits) answer = ollama_generate(prompt, model=LLM_MODEL) return {"answer": answer, "evidence": hits}

======================== CLI ========================

def main(): print("ğŸ“¥ ×˜×•×¢×Ÿ ×˜×‘×œ×” ×-MSSQL...") df = load_sales_table() print(f"âœ… × ×˜×¢× ×• {len(df)} ×©×•×¨×•×ª, {len(df.columns)} ×¢××•×“×•×ª.")

print("ğŸ—ï¸ ×‘×•× ×”/×˜×•×¢×Ÿ ××™× ×“×§×¡ FAISS ××§×•××™...")
index, df_meta, dim = ensure_index(df, force_rebuild=False)
print(f"âœ… ××™× ×“×§×¡ ××•×›×Ÿ. dim={dim}, items={index.ntotal}")

print("\n××¤×©×¨ ×œ×©××•×œ ×©××œ×•×ª (×›×ª×•×‘ 'exit' ×œ×™×¦×™××”):")
while True:
    q = input("\n×©××œ×”: ").strip()
    if q.lower() in ("exit", "quit"): break
    out = answer_question(q, index, df_meta, top_k=6)
    print("\nâ€” ×ª×©×•×‘×” â€”\n" + out["answer"]) 
    print("\n(××¡××›×™ ×”×§×©×¨):")
    for i, h in enumerate(out["evidence"], 1):
        print(f" {i}. score={h['score']:.3f} | {h['text'][:160]}...")

if name == "main": main()

    texts = df.astype(str).agg(" ".join, axis=1).tolist()
    embeddings = model.encode(texts, convert_to_numpy=True)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index, texts, model

# ===== ×—×™×¤×•×© ×‘×˜×‘×œ×” =====
def search(query, index, texts, model, top_k=5):
    query_emb = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_emb, top_k)
    return [texts[i] for i in indices[0]]

# ===== ×©×œ×™×—×ª ×©××œ×” ×œ-Ollama =====
def ollama_generate(prompt, model="mistral"):
    cmd = ["ollama", "run", model]
    process = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    out, err = process.communicate(input=prompt)
    return out.strip()

# ===== ×¤×•× ×§×¦×™×” ×œ×©××œ×•×ª =====
def answer_question(query, index, texts, model):
    results = search(query, index, texts, model)
    context = "\n".join(results)
    prompt = (
        "×”× ×š ×¢×•×–×¨ ×—×›× ×œ× ×™×ª×•×— × ×ª×•× ×™ ××›×™×¨×•×ª. "
        "×”×©×ª××© ××š ×•×¨×§ ×‘× ×ª×•× ×™× ×”×‘××™× ×›×“×™ ×œ×¢× ×•×ª:\n"
        f"{context}\n\n"
        f"×©××œ×”: {query}\n"
        "×ª×©×•×‘×”:"
    )
    return ollama_generate(prompt, model="mistral")  # ××¤×©×¨ ×œ×”×—×œ×™×£ ×œ-llama3

# ===== ×”×¨×¦×” =====
if __name__ == "__main__":
    print("ğŸ“¥ ×˜×•×¢×Ÿ ××ª ×˜×‘×œ×ª ×”××›×™×¨×•×ª...")
    df = load_sales_table()
    index, texts, emb_model = build_faiss_index(df)
    print("âœ… ×”××¢×¨×›×ª ××•×›× ×”. ××¤×©×¨ ×œ×©××•×œ ×©××œ×•×ª!")

    while True:
        q = input("\n×”×›× ×¡ ×©××œ×” (××• 'exit' ×œ×™×¦×™××”): ")
        if q.lower() == "exit":
            break
        answer = answer_question(q, index, texts, emb_model)
        print("\nğŸ’¡ ×ª×©×•×‘×”:", answer)print(f"× ×˜×¢× ×• {len(df)} ×©×•×¨×•×ª ×•-{len(df.columns)} ×¢××•×“×•×ª.")

# ===== ×”××¨×ª ×©×•×¨×•×ª ×œ×˜×§×¡×˜×™× =====
def row_to_text(row):
    return "; ".join(f"{col}: {row[col]}" for col in df.columns)

docs = [row_to_text(r) for _, r in df.iterrows()]

# ===== ×™×¦×™×¨×ª Embeddings =====
EMBED_MODEL_NAME = "all-MiniLM-L6-v2"
embedder = SentenceTransformer(EMBED_MODEL_NAME)
embeddings = embedder.encode(docs, convert_to_numpy=True)

# ===== ×‘× ×™×™×ª ××™× ×“×§×¡ FAISS =====
dim = embeddings.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(embeddings)
print(f"××™× ×“×§×¡ FAISS × ×‘× ×” ×¢× {index.ntotal} ×¤×¨×™×˜×™×.")

# ===== ×˜×¢×™× ×ª ××•×“×œ ×©×¤×” ××§×•××™ =====
LOCAL_LLM_MODEL = "mistralai/Mistral-7B-Instruct-v0.1"  # ××¤×©×¨ ×œ×”×—×œ×™×£ ×‘××•×“×œ ××—×¨ ×©×”×•×¨×“×ª
llm = pipeline("text-generation", model=LOCAL_LLM_MODEL, device_map="auto")

# ===== ×¤×•× ×§×¦×™×™×ª ×—×™×¤×•×© =====
def search(query, top_k=5):
    q_emb = embedder.encode([query], convert_to_numpy=True)
    D, I = index.search(q_emb, top_k)
    results = []
    for dist, idx in zip(D[0], I[0]):
        results.append({
            "score": float(dist),
            "text": docs[idx],
            "row": df.iloc[idx].to_dict()
        })
    return results

# ===== ×¤×•× ×§×¦×™×” ×œ×¢× ×™×™×ª ×©××œ×•×ª (RAG) =====
def answer_question(query, top_k=5):
    results = search(query, top_k)
    context = "\n".join([r["text"] for r in results])
    prompt = (
        "×”× ×š ×¢×•×–×¨ ×—×›× ×œ××›×™×¨×•×ª. "
        "×”× ×” × ×ª×•× ×™ ×”×”×§×©×¨:\n"
        f"{context}\n"
        f"×©××œ×”: {query}\n"
        "×¢× ×” ×‘×¦×•×¨×” ××¤×•×¨×˜×ª ×•××‘×•×¡×¡×ª ×¢×œ ×”× ×ª×•× ×™× ×‘×œ×‘×“."
    )
    resp = llm(prompt, max_length=512, do_sample=False)
    return resp[0]["generated_text"]

# ===== ×××©×§ CLI =====
while True:
    q = input("\n×©××œ×” ('quit' ×œ×™×¦×™××”): ").strip()
    if q.lower() in ("quit", "exit"):
        break
    answer = answer_question(q, top_k=5)
    print("\n--- ×ª×©×•×‘×” ---")
    print(answer)
# ===== ×¤×•× ×§×¦×™×™×ª ×—×™×¤×•×© =====
def search(query, top_k=5):
    q_emb = model.encode([query], convert_to_numpy=True)
    D, I = index.search(q_emb, top_k)
    results = []
    for dist, idx in zip(D[0], I[0]):
        results.append({
            "score": float(dist),
            "row": df.iloc[idx].to_dict()
        })
    return results

# ===== × ×™×¡×•×™ ×—×™×¤×•×© =====
while True:
    q = input("\n×©××œ×” ××• ×—×™×¤×•×© ('quit' ×œ×™×¦×™××”): ").strip()
    if q.lower() in ("quit", "exit"):
        break
    hits = search(q, top_k=3)
    for h in hits:
        print(f"\n×¦×™×•×Ÿ: {h['score']:.4f}")
        for k, v in h["row"].items():
            print(f"{k}: {v}")
